{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0933f239",
   "metadata": {},
   "source": [
    "# **ML Assignment 10**\n",
    "\n",
    "By 23520011 - Sharaneshwar Punjal\n",
    "\n",
    "Download the following dataset - https://www.kaggle.com/datasets/erdemtaha/cancer-data/data \n",
    "\n",
    "1. Drop `Id` column.\n",
    "\n",
    "2. Use the `Diagnosis` column as the target with Classes B and M.\n",
    "\n",
    "3. Perform a train-test split: 80% for training and 20% for testing.\n",
    "\n",
    "4. Following manipulation is performed to increase the skew in the data (only for this assignment, not to be done in practice):\n",
    "   - From train data:\n",
    "      - Consider all rows with diagnosis label = M.\n",
    "      - From these, randomly remove 120 rows with label M and append them to the test data.\n",
    "\n",
    "5. Build 10 decision trees using:\n",
    "   - Feature bagging: Use all features for each tree, but at each node split, randomly choose a subset of features using `max_features` in sklearn.\n",
    "   - Sample bagging: If the train data size is N, train each tree using N samples selected with replacement.\n",
    "\n",
    "6. Combine feature importance from all trees:\n",
    "   - Either use simple average or a weighted average using tree accuracy as weight.\n",
    "   - Feature importance can be fetched from `.feature_importances_` attribute or computed using `sklearn.inspection.permutation_importance`.\n",
    "\n",
    "7. Shortlist the most important features based on the above step and drop the rest from training data.\n",
    "\n",
    "8. Train 10 new decision trees using only the shortlisted features.\n",
    "\n",
    "9. Build two models using inputs as:\n",
    "   - Shortlisted features\n",
    "   - Predictions from the 10 trees trained in step 8\n",
    "      - Logistic Regression Model\n",
    "      - Master Decision Tree\n",
    "\n",
    "10. On test data:\n",
    "   - Keep only the shortlisted features.\n",
    "   - Predict using the 10 trees from step 8.\n",
    "   - Combine these predictions with the shortlisted features and:\n",
    "      - Predict using the logistic regression model.\n",
    "      - Predict using the master decision tree.\n",
    "   - Compare accuracies of both models and evaluate which gives better performance. Note how much improvement they offer over the original 10 trees.\n",
    "\n",
    "11. Since the training data was made more skewed by reducing the minority class (M), use class weights while training. Also, while evaluating accuracy, pay special attention to recall of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b95c09-8b4f-4b65-a3d7-d0e0af9f549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47957e5a-6092-4472-967a-b1dcee723f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Cancer_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "689a3918-f0fb-4ea5-8c40-5684e5070202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id'])\n",
    "df = df.drop(columns=['Unnamed: 32'])\n",
    "\n",
    "# Separate features and target with binary mapping: 'B' → 0, 'M' → 1\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis'].map({'B': 0, 'M': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdb74a-c8d1-461a-84fa-f36acfee9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2 , random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418ca915-8a3f-4eb1-a8b6-3692a2bede6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X and y to filter together\n",
    "train_data = X_train.copy()\n",
    "train_data['Diagnosis'] = y_train\n",
    "\n",
    "# Filter rows where Diagnosis == 1 (Malignant)\n",
    "m_samples = train_data[train_data['Diagnosis'] == 1]\n",
    "\n",
    "# Randomly select 120 rows with label 1 (Malignant)\n",
    "m_selected = m_samples.sample(n=120, random_state=42)\n",
    "\n",
    "# Drop these rows from training data\n",
    "train_data = train_data.drop(m_selected.index)\n",
    "\n",
    "# Prepare test data\n",
    "test_data = X_test.copy()\n",
    "test_data['Diagnosis'] = y_test\n",
    "\n",
    "# Append the 120 'Malignant' rows to test data\n",
    "test_data = pd.concat([test_data, m_selected])\n",
    "\n",
    "# Separate features and target again\n",
    "X_train = train_data.drop('Diagnosis', axis=1)\n",
    "y_train = train_data['Diagnosis']\n",
    "\n",
    "X_test = test_data.drop('Diagnosis', axis=1)\n",
    "y_test = test_data['Diagnosis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a8dacc-b5bd-492d-bd8e-636f891ad24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "trees = []\n",
    "for i in range(10):\n",
    "    # 1. Bootstrap sampling\n",
    "    X_sample, y_sample = resample(X_train, y_train, replace=True, n_samples=len(X_train), random_state=i)\n",
    "\n",
    "    # 2. Train tree with feature bagging\n",
    "    tree = DecisionTreeClassifier(max_features='sqrt', random_state=i)\n",
    "    tree.fit(X_sample, y_sample)\n",
    "\n",
    "    trees.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe43e8c-f19f-433d-a05b-25070a195a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(trees[i], filled=True, feature_names=X.columns, class_names=[\"Benign\", \"Malignant\"], rounded=True)\n",
    "    plt.title(f\"Decision Tree {i+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3126744f-d6af-407b-8636-c9bfc199da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius_mean                     Simple Avg: 0.0177  Weighted Avg: 0.0167\n",
      "texture_mean                    Simple Avg: 0.0231  Weighted Avg: 0.0230\n",
      "perimeter_mean                  Simple Avg: 0.0049  Weighted Avg: 0.0049\n",
      "area_mean                       Simple Avg: 0.0000  Weighted Avg: 0.0000\n",
      "smoothness_mean                 Simple Avg: 0.0054  Weighted Avg: 0.0054\n",
      "compactness_mean                Simple Avg: 0.0067  Weighted Avg: 0.0067\n",
      "concavity_mean                  Simple Avg: 0.0244  Weighted Avg: 0.0246\n",
      "concave points_mean             Simple Avg: 0.0752  Weighted Avg: 0.0774\n",
      "symmetry_mean                   Simple Avg: 0.0002  Weighted Avg: 0.0002\n",
      "fractal_dimension_mean          Simple Avg: 0.0000  Weighted Avg: 0.0000\n",
      "radius_se                       Simple Avg: 0.0087  Weighted Avg: 0.0088\n",
      "texture_se                      Simple Avg: 0.0000  Weighted Avg: 0.0000\n",
      "perimeter_se                    Simple Avg: 0.0043  Weighted Avg: 0.0040\n",
      "area_se                         Simple Avg: 0.0012  Weighted Avg: 0.0012\n",
      "smoothness_se                   Simple Avg: 0.0000  Weighted Avg: 0.0000\n",
      "compactness_se                  Simple Avg: 0.0085  Weighted Avg: 0.0086\n",
      "concavity_se                    Simple Avg: 0.0071  Weighted Avg: 0.0071\n",
      "concave points_se               Simple Avg: 0.0044  Weighted Avg: 0.0040\n",
      "symmetry_se                     Simple Avg: 0.0028  Weighted Avg: 0.0029\n",
      "fractal_dimension_se            Simple Avg: 0.0066  Weighted Avg: 0.0066\n",
      "radius_worst                    Simple Avg: 0.1861  Weighted Avg: 0.1901\n",
      "texture_worst                   Simple Avg: 0.0121  Weighted Avg: 0.0125\n",
      "perimeter_worst                 Simple Avg: 0.0685  Weighted Avg: 0.0687\n",
      "area_worst                      Simple Avg: 0.1584  Weighted Avg: 0.1581\n",
      "smoothness_worst                Simple Avg: 0.0136  Weighted Avg: 0.0140\n",
      "compactness_worst               Simple Avg: 0.0082  Weighted Avg: 0.0081\n",
      "concavity_worst                 Simple Avg: 0.0154  Weighted Avg: 0.0158\n",
      "concave points_worst            Simple Avg: 0.3164  Weighted Avg: 0.3103\n",
      "symmetry_worst                  Simple Avg: 0.0175  Weighted Avg: 0.0177\n",
      "fractal_dimension_worst         Simple Avg: 0.0026  Weighted Avg: 0.0026\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming: you have X_test, y_test already split\n",
    "n_features = X.shape[1]\n",
    "feature_names = X.columns\n",
    "\n",
    "# Store feature importances and accuracy\n",
    "importances = []\n",
    "accuracies = []\n",
    "\n",
    "for tree in trees:\n",
    "    # 1. Get feature importance\n",
    "    importances.append(tree.feature_importances_)\n",
    "\n",
    "    # 2. Compute accuracy of this tree\n",
    "    y_pred = tree.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "importances = np.array(importances)  # shape: (10, num_features)\n",
    "accuracies = np.array(accuracies)    # shape: (10,)\n",
    "\n",
    "# ---------------------------\n",
    "# Simple average\n",
    "avg_importance = np.mean(importances, axis=0)\n",
    "\n",
    "# Weighted average using accuracy\n",
    "weighted_importance = np.average(importances, axis=0, weights=accuracies)\n",
    "\n",
    "# ---------------------------\n",
    "# Show feature rankings\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"{name:<30}  Simple Avg: {avg_importance[i]:.4f}  Weighted Avg: {weighted_importance[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1affc-ad5b-4a11-bca9-007dfe242b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_n = 10\n",
    "indices = np.argsort(weighted_importance)[-top_n:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(top_n), weighted_importance[indices], align='center')\n",
    "plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Weighted Feature Importance')\n",
    "plt.title('Top 10 Features (Weighted Avg)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70fae92b-5cbc-47a2-a1a6-45d580506fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top features: ['concavity_worst', 'radius_mean', 'symmetry_worst', 'texture_mean', 'concavity_mean', 'perimeter_worst', 'concave points_mean', 'area_worst', 'radius_worst', 'concave points_worst']\n"
     ]
    }
   ],
   "source": [
    "# Number of top features to keep\n",
    "top_k = 10\n",
    "\n",
    "# Get indices of top features based on weighted importance\n",
    "top_indices = np.argsort(weighted_importance)[-top_k:]\n",
    "\n",
    "# Get their names\n",
    "selected_features = [feature_names[i] for i in top_indices]\n",
    "\n",
    "print(\"Selected top features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4e1476f-d17d-4314-9ff3-3304045b35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4cf6b-839f-4dd5-965c-b6393e3785b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train new trees using only selected (shortlisted) features\n",
    "trees_selected = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Sample N rows with replacement (bootstrap sampling)\n",
    "    indices = np.random.choice(len(X_train_selected), size=len(X_train_selected), replace=True)\n",
    "    X_sample = X_train_selected.iloc[indices]\n",
    "    y_sample = y_train.iloc[indices]\n",
    "\n",
    "    # Train tree using random feature selection at each split\n",
    "    clf = DecisionTreeClassifier(max_features='sqrt', random_state=i)\n",
    "    clf.fit(X_sample, y_sample)\n",
    "\n",
    "    trees_selected.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c7b30-e37e-4d39-8e24-48d6d09b7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Train new trees using only selected (shortlisted) features\n",
    "trees_selected = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Bootstrap sample the training data\n",
    "    indices = np.random.choice(len(X_train_selected), size=len(X_train_selected), replace=True)\n",
    "    X_sample = X_train_selected.iloc[indices]\n",
    "    y_sample = y_train.iloc[indices]\n",
    "\n",
    "    # Train the decision tree with random feature selection at each node\n",
    "    clf = DecisionTreeClassifier(max_features='sqrt', random_state=i)\n",
    "    clf.fit(X_sample, y_sample)\n",
    "    trees_selected.append(clf)\n",
    "\n",
    "    # Visualize the tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(clf, filled=True, feature_names=X_train_selected.columns, class_names=[\"Benign\", \"Malignant\"], rounded=True)\n",
    "    plt.title(f\"Decision Tree {i+1} (Shortlisted Features)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea67956-1456-4dfd-862e-6be3bc168c77",
   "metadata": {},
   "source": [
    "#### Step 1: Get Tree Predictions on Train Data\n",
    "We’ll generate 10 new columns (tree_1, ..., tree_10) from the previous 10 trees’ predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529dc3dd-4bcc-4422-a1b3-a4e3804971cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions of each tree on the training data\n",
    "tree_outputs_train = []\n",
    "\n",
    "for tree in trees_selected:\n",
    "    preds = tree.predict(X_train_selected)\n",
    "    tree_outputs_train.append(preds)\n",
    "\n",
    "# Convert list of arrays to a DataFrame (transpose to make columns)\n",
    "tree_outputs_train = np.array(tree_outputs_train).T\n",
    "tree_output_df_train = pd.DataFrame(tree_outputs_train, columns=[f\"tree_{i+1}\" for i in range(10)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1a050-fe25-484a-b7fa-ad80736f6506",
   "metadata": {},
   "source": [
    "#### Step 2: Concatenate with Shortlisted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed82946-b342-4b1f-9329-4d803449d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate tree outputs with selected original features\n",
    "X_meta_train = pd.concat([X_train_selected.reset_index(drop=True), tree_output_df_train], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9ee5b-3987-4580-8cc4-9137b6aa123a",
   "metadata": {},
   "source": [
    "#### Step 3: Repeat for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3b9cc5e-0b36-48e6-bb9c-365c31eba5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_outputs_test = []\n",
    "\n",
    "for tree in trees_selected:\n",
    "    preds = tree.predict(X_test_selected)\n",
    "    tree_outputs_test.append(preds)\n",
    "\n",
    "tree_outputs_test = np.array(tree_outputs_test).T\n",
    "tree_output_df_test = pd.DataFrame(tree_outputs_test, columns=[f\"tree_{i+1}\" for i in range(10)])\n",
    "\n",
    "X_meta_test = pd.concat([X_test_selected.reset_index(drop=True), tree_output_df_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ee84c-8b2e-4864-a7ca-8286a11c2bab",
   "metadata": {},
   "source": [
    "#### Step 4a: Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d55a5a9-34f5-466b-a780-4014f202eb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9188034188034188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression with class weight 'balanced'\n",
    "log_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "log_model.fit(X_meta_train, y_train)\n",
    "\n",
    "y_pred_log = log_model.predict(X_meta_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb085b6-9d65-484c-9872-67aeaea75399",
   "metadata": {},
   "source": [
    "#### Step 4b: Train Master Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a46c78-7aca-4fa6-9b1f-b243cca49e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Decision Tree Accuracy: 0.8846153846153846\n"
     ]
    }
   ],
   "source": [
    "# Master Decision Tree with class weight 'balanced'\n",
    "master_tree = DecisionTreeClassifier(max_features='sqrt', class_weight='balanced', random_state=42)\n",
    "master_tree.fit(X_meta_train, y_train)\n",
    "\n",
    "y_pred_master = master_tree.predict(X_meta_test)\n",
    "print(\"Master Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_master))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f829c-f947-4baa-bb21-1f942258a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(master_tree, filled=True, feature_names=X_meta_train.columns, class_names=[\"Benign\", \"Malignant\"], rounded=True)\n",
    "plt.title(\"Master Decision Tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8d94f-2cf9-42d3-8a60-48135c736c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, classification_report\n",
    "\n",
    "# --- a. Just retain the shortlisted features from test set ---\n",
    "X_test_selected = X_test[selected_features]  # already defined 'selected_features'\n",
    "\n",
    "# --- b. Make predictions using the 10 decision trees ---\n",
    "tree_preds_test = []\n",
    "\n",
    "for tree in trees_selected:  # assuming 'trees_selected' = list of 10 trained trees on selected features\n",
    "    preds = tree.predict(X_test_selected)\n",
    "    tree_preds_test.append(preds)\n",
    "\n",
    "tree_preds_test = np.array(tree_preds_test).T  # shape: (n_samples, 10)\n",
    "tree_output_df_test = pd.DataFrame(tree_preds_test, columns=[f'tree_{i+1}' for i in range(10)])\n",
    "\n",
    "# --- c. Combine tree outputs with shortlisted features for meta-models ---\n",
    "X_meta_test = pd.concat([X_test_selected.reset_index(drop=True), tree_output_df_test], axis=1)\n",
    "\n",
    "# --- i. Predict using logistic regression model ---\n",
    "log_pred = log_model.predict(X_meta_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, log_pred))\n",
    "print(\"Logistic Regression Recall (minority class):\", recall_score(y_test, log_pred))\n",
    "print(classification_report(y_test, log_pred))\n",
    "\n",
    "# --- ii. Predict using master decision tree model ---\n",
    "master_pred = master_tree.predict(X_meta_test)\n",
    "print(\"Master Decision Tree Accuracy:\", accuracy_score(y_test, master_pred))\n",
    "print(\"Master Decision Tree Recall (minority class):\", recall_score(y_test, master_pred))\n",
    "print(classification_report(y_test, master_pred))\n",
    "\n",
    "# --- d. Evaluate baseline accuracy from individual trees ---\n",
    "tree_accuracies = [accuracy_score(y_test, tree_output_df_test[f'tree_{i+1}']) for i in range(10)]\n",
    "avg_tree_accuracy = sum(tree_accuracies) / 10\n",
    "print(f\"Average Accuracy of 10 Decision Trees: {avg_tree_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
